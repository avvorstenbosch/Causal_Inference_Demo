---
title: 'Causal Inference Demo'
author: "Alex van Vorstenbosch"
date: "`r format(Sys.time(), '%d %B, %Y')`"
fontsize: 10pt
output:
  beamer_presentation: 
    theme: "hannover"
    colortheme: "dolphin"
header-includes:
  - \usepackage{tikz}
  - \usepackage{multicol}
  - \usepackage{hyperref}
---

```{r setup II, include=FALSE, eval=TRUE}
knitr::opts_chunk$set(eval = TRUE, cache=TRUE, echo = FALSE, warning = FALSE, message = FALSE, dev = 'pdf')
```

```{r packages}
library(tidyverse)
library(dagitty)
library(brms) # bayesian analyse
library(broom) # voor tidy regressie resultaten
library(broom.mixed) # broom voor brms
library(tidybayes) # voor extracting samples
library(ggridges) # voor ggridges plots
set.seed(300)
```


# What are we going to see

* Defining the problem
* Building our DAG
* Simulating the data
* Can we use BRMS to retreive the parameters?
* How does the model do when fitting \textbf{Real World Data...}
* Appendix

# Defining the Problem

* Mathematics is on of THE fundamental school subject...
* ...yet, it is stereotypically seen as boring and difficult.
* In this demo we explore what influences school performance in Math
* ... and possible intervention paths we could use.
* As outcome variable we take the end of year grade in Mathematics for high school students.

# DAG - The Full Causal Pentagram
```{r fig1, out.width="80%", fig.align = "center"}
dag <- dagitty('dag {
bb="-4.361,-3.907,6.053,5.052"
EducationParents [pos="0.165,-2.506"]
ExtraEducationSupport [pos="-2.624,0.509"]
Grade [outcome,pos="0.128,0.557"]
HigherEducationAspiration [pos="-1.235,-0.496"]
InternetAcces [pos="-0.979,-1.746"]
PaidCourses [pos="-0.488,2.606"]
RegionHouse [pos="-2.460,-2.562"]
Sex [pos="2.442,-1.390"]
SocietalBias [latent,pos="1.391,-0.504"]
StudyTime [pos="-2.019,2.583"]
EducationParents -> Grade
EducationParents -> HigherEducationAspiration
ExtraEducationSupport -> Grade
ExtraEducationSupport -> PaidCourses
HigherEducationAspiration -> ExtraEducationSupport
HigherEducationAspiration -> Grade
HigherEducationAspiration -> PaidCourses
HigherEducationAspiration -> StudyTime
InternetAcces -> Grade
PaidCourses -> Grade
RegionHouse -> EducationParents
RegionHouse -> HigherEducationAspiration
RegionHouse -> InternetAcces
Sex -> SocietalBias
SocietalBias -> Grade
SocietalBias -> HigherEducationAspiration
StudyTime -> ExtraEducationSupport
StudyTime -> Grade
StudyTime -> PaidCourses
}
')
plot(dag)
```
The unmeasured variable SocietalBias explains how the student's Sex effects Grade and HigherEducationAspiration.
However, since SocietalBias is only a 'Mechanism', we can remove it from the DAG and simplify it. 


<!--
This causal model is one of the mental models one might design for features influencing final grades.
We are not saying this model is correct. In fact, this model might communicate a rather simplistic view.

One might add many more features, but this will over complicate the model, even this DAG is already rather complex.
The most important part is that we communicate this DAG, as it informs other on the assumptions behind our models.

Important to note is that a subject such as making a DAG, and as a consequence simulating data from this Data Generating Process (DGP),
might feel like a sensitive subject. Quite rightfully so. Just remember, it feels sensitive because you are being very explicit about assumptions, which makes you vulnerable.
In any other analysis you might have done, you bake in the same set of assumptions, but you don't make them as explicit, giving a false sense of protection and safety!
-->

# DAG - The (slightly simplified) Causal Pentagram
```{r fig2, out.width="80%", fig.align = "center"}
dag <- dagitty('dag {
bb="-4.361,-3.907,6.053,5.052"
EducationParents [pos="0.165,-2.506"]
ExtraEducationSupport [pos="-2.624,0.509"]
Grade [outcome,pos="0.128,0.557"]
HigherEducationAspiration [pos="-1.235,-0.496"]
InternetAcces [pos="-0.979,-1.746"]
PaidCourses [pos="-0.488,2.606"]
RegionHouse [pos="-2.460,-2.562"]
Sex [pos="2.442,-1.390"]
StudyTime [pos="-2.019,2.583"]
EducationParents -> Grade
EducationParents -> HigherEducationAspiration
ExtraEducationSupport -> Grade
ExtraEducationSupport -> PaidCourses
HigherEducationAspiration -> ExtraEducationSupport
HigherEducationAspiration -> Grade
HigherEducationAspiration -> PaidCourses
HigherEducationAspiration -> StudyTime
InternetAcces -> Grade
PaidCourses -> Grade
RegionHouse -> EducationParents
RegionHouse -> HigherEducationAspiration
RegionHouse -> InternetAcces
Sex -> Grade
Sex -> HigherEducationAspiration
StudyTime -> ExtraEducationSupport
StudyTime -> Grade
StudyTime -> PaidCourses
}
')
plot(dag)
```
This DAG is slightly simpler, but the 'Causal Pentagram' looks like it might cause problems.
Time to find out!

# Simulate data - 1

\tiny
```{r, echo=TRUE}
#Starting in the upper half of the DAG
N=500
RegionHouse = sample(x = c("R", "U"), size=N, replace=TRUE, prob=c(0.3,0.7))

EducationParents = ifelse(RegionHouse=="U",
                          sample(x = 0:1, size=N, replace=TRUE, prob=c(0.4,0.6)),
                          sample(x = 0:1, size=N, replace=TRUE, prob=c(0.6,0.4)))

InternetAcces = ifelse(RegionHouse=="U",
                          sample(x = c(TRUE,FALSE), size=N, replace=TRUE, prob=c(0.85,0.15)),
                          sample(x = c(TRUE,FALSE), size=N, replace=TRUE, prob=c(0.95,0.05)))

Sex = sample(x = c("M", "F"), size=N, replace=TRUE, prob=c(0.5,0.5))
```
\normalsize

# Simulate data - 2

\tiny
```{r, echo=TRUE}

CalcHigherEducation <- function(RegionHouse, EducationParents, Sex){
  # Samples HigherEducationAspiration based on 3 inputs.
  # Assumption: Growing up in a rural region you may be less motivated to go to college.
  # Assumption: Due to societal pressure, women might be less likely to persue college. 
  HEA_base = 0.6
  Sex_effect = ifelse(Sex=="M", 0.05, -0.05)
  EducationParents_effect = ifelse(Sex==1,-0.1,0.1)
  RegionHouse_effect = ifelse(RegionHouse=="R", -0.05, 0.05)
  HEA_p = HEA_base + Sex_effect + EducationParents_effect + RegionHouse_effect
  HEA = rbernoulli(n=length(HEA_p) ,p=HEA_p)
  return(HEA)
}
HigherEducationAspiration = CalcHigherEducation(RegionHouse, EducationParents, Sex)

StudyTime = ifelse(HigherEducationAspiration==1,
                   sample(x = 0:3, size=N, replace=TRUE, prob=c(0.15,0.15,0.35,0.35)),
                   sample(x = 0:3, size=N, replace=TRUE, prob=c(0.3,0.35,0.25,0.1))
)

```
\normalsize

# Simulate data - 3

\tiny
```{r, echo=TRUE}
CalcExtraEducationSupport <- function(HigherEducationAspiration, StudyTime){
  # Samples ExtraEducationSupport based on 2 inputs
  # Assumption: If you are good at doing homework yourself (more hours), you get less support
  # Assumption: If you are motivated to go to college, you might seek out more support even if you study a lot
  EAS_base = 0.2
  HEA_effect = ifelse(HigherEducationAspiration==1, 0.075, -0.05)
  StudyTime_effect = -0.025*StudyTime
  EAS_p = EAS_base + HEA_effect + StudyTime_effect
  EAS = rbernoulli(n=length(EAS_p) ,p=EAS_p)
  return(EAS)
}
ExtraEducationalSupport = CalcExtraEducationSupport(HigherEducationAspiration, StudyTime)
  
CalcPaidCourses <- function(StudyTime, HigherEducationAspiration, ExtraEducationalSupport){
  # Samples PaidCourses based on 3 inputs
  # Assumption: If you study little, and get no extra support, but want to go to college,
  #             you might try to catch up using paid courses
  # Assumption: The opposite might also be true, college + many hours studying + extra support,
  #             may mean the student is more likely to also pay for extra courses
  Paid_base = 0.05
  effect = rep(0, length(StudyTime))
  for (i in length(StudyTime)){
    if(StudyTime[i]<2 & HigherEducationAspiration[i]==1 & ExtraEducationalSupport[i]==0){
      effect[i] = 0.1
    } else if(StudyTime[i]>3 & HigherEducationAspiration[i]==1 & ExtraEducationalSupport[i]==1){
      effect[i] = 0.1
    }
  }
  Paid_p = Paid_base + effect
  Paid = rbernoulli(n=length(Paid_p) ,p=Paid_p)
}
PaidCourses = CalcPaidCourses(StudyTime, HigherEducationAspiration, ExtraEducationalSupport)
  
```
\normalsize

# True weights for calculating grades

```{r, echo=TRUE}
# Finally, calculate grade
set.seed(925)
Grade = rnorm(n=500, mean=5.0, sd=0.3) + 
  1*PaidCourses +
  0.5*StudyTime + 
  1*ExtraEducationalSupport +
  0.7*HigherEducationAspiration +
  0.6*InternetAcces -0.6 + #convenient way to include negative effect
  ifelse(EducationParents==1, 0.5, -1) +
  ifelse(Sex=="F", -0.25, 0.25)
```

# Simulated grades distribution

```{r fig3, out.width="80%", fig.align = "center"}
plot_data = data.frame(Grade)
ggplot(plot_data) + 
  geom_histogram(aes(x=Grade), closed="left", fill="royalblue1", color='white', breaks=seq(1.0,10,0.5), show.legend=TRUE) + 
  labs(title=paste("Distribution of 500 Simulated Math Grades"), subtitle="By Alex", x="Grade", y="Counts (#)") +
  scale_x_continuous(breaks=seq(1,10,1))
```
In our simulation, Most students score between a 5.5 and a 6. This seems reasonable.
Also, very high scores are rare, as well as extremely low scores. The distribution is roughly normal, but not quite. 
\normalsize

# Retrieve these direct effects using BRMS

```{r}
fullrun <- 1

# We add 0 + intercept in order to stop BRMS from centering the model, now we can properly fit Intercept as in the original simulation
Formula <- Grade ~ 0  + Intercept + 
                        PaidCourses +
                        StudyTime +
                        ExtraEducationalSupport +
                        HigherEducationAspiration +
                        InternetAcces + 
                        EducationParents +
                        Sex 
    
sim_data <- data.frame(PaidCourses,
                        StudyTime,
                        ExtraEducationalSupport,
                        HigherEducationAspiration,
                        InternetAcces,
                        EducationParents,
                        Sex, 
                        Grade)

priors <- c(set_prior("normal(5.5, 0.5)", class = "b", coef = "Intercept"),
            set_prior("normal(0, 0.5)", class = "b", coef = "PaidCoursesTRUE"),
            set_prior("normal(0, 0.5)", class = "b", coef = "StudyTime"),
            set_prior("normal(0, 0.5)", class = "b", coef = "ExtraEducationalSupportTRUE"),
            set_prior("normal(0, 0.5)", class = "b", coef = "HigherEducationAspirationTRUE"),
            set_prior("normal(0, 0.5)", class = "b", coef = "InternetAccesTRUE"),
            set_prior("normal(0, 0.5)", class = "b", coef = "EducationParents"),
            set_prior("normal(0, 0.5)", class = "b", coef = "SexM"))

if(fullrun){
  model_math <- brm(
              formula = Formula,
              prior = priors,
              data   = sim_data,
              warmup = 500, 
              iter   = 10000, 
              chains = 6, 
              inits  = "random",
              cores  = 6,
              seed = 123,
              backend = "cmdstanr",
              control = list(adapt_delta = 0.99, 
                             max_treedepth = 17),
              silent = TRUE,
              refresh = 0)
  
  saveRDS(model_math, "./models/model_demo.rds")
} else {
  model_math <- readRDS("./models/model_demo.rds")
}
```

# Checking our posterior and chains!

```{r fig4, out.width="80%", fig.align = "center"}
model_math %>%
  plot(combo = c("hist", "trace"), widths = c(1, 1.5),
       theme = theme_bw(base_size = 10),
       ask   = FALSE
       )
  
```
Good mixing, and clean normal estimates for all parameters!
```{r}
summary(model_math)
```

```{r}
# now we sample from the model in order to show our predictions
samples_math <- tidy_draws(model_math) %>%
  dplyr::select(starts_with("b_")) %>%
  pivot_longer(cols = 1:8, names_to = "Model1", values_to = "Value") %>%
  mutate(Feature = as.factor(str_sub(Model1, 3))) %>%
  dplyr::select(Feature, Value)

real_vals <- data.frame(params = c("StudyTime", "SexM", "PaidCoursesTRUE", "InternetAccesTRUE", "Intercept", "HigherEducationAspirationTRUE", "ExtraEducationalSupportTRUE", "EducationParents"),
                       True_Value = c(0.5, 0.5, 1, 0.6, 3.15, 0.7, 1, 1.5))
                       
samples_math <- left_join(samples_math, real_vals, by = c("Feature" = "params")) 
                       
```

# Bayesian Parameter Estimates 

```{r fig5, out.width="80%", fig.align = "center"}
ggplot(samples_math, aes(x=Value, y=Feature, fill=Feature)) +
  geom_density_ridges2(scale = 0.9, 
                       quantile_lines=TRUE,
                       quantile_fun=function(x,...)mean(x)) +
  scale_fill_viridis_d() +
  geom_segment(data=samples_math,aes(x = True_Value,
                   xend = True_Value,
                   y = as.numeric(as.factor(Feature)),
                   yend = as.numeric(as.factor(Feature))+1),
               color = "red", fill="red") +
  theme(legend.position = "none") +
  labs(y = " ", x = "coefficient estimates", title="Bayesian parameter estimates") + 
  scale_x_continuous(breaks=seq(0,3.25,0.25))
```
Almost all Parameter values are perfectly retrieved! PaidCourses is slightly underestimated,
And Sex is slighty overestimated. But these results are very promising

# How does the model perform in terms of predictions?
```{r}
preds_raw <- as_tibble(predict(model_math, newdata = sim_data))

preds <- cbind(sim_data, preds_raw)

plot_data = preds %>%
            mutate(pred_min=Estimate-Est.Error,
                   pred_max=Estimate+Est.Error)
```

```{r fig6, out.width="80%", fig.align = "center"}
ggplot() +
    geom_errorbar(data = plot_data, aes(x = Grade, xend = Grade, ymin=pred_min, ymax = pred_max), size=0.1, width=0.1, color = "red") +
  geom_point(data = plot_data, aes(x = Grade, y = Estimate)) +
  geom_abline(intercept = 0, slope = 1, color = "firebrick", linetype = "dashed") +
  labs(y = "Predicted Grade", x = "True Grade", title="Grade vs. Predicted Grade")+
  scale_x_continuous(breaks=seq(1,10,1)) +
  scale_y_continuous(breaks=seq(1,10,1)) +
  coord_fixed()


```
* This looks good! Most estimates overlap the true value within 1 sigma deviation.
* Estimates seem consistent across the whole domain

# Time for real data
* We'll use \textcolor{cyan}{https://www.kaggle.com/datasets/dipam7/student-grade-prediction}
* This dataset contains portugese data on Math grades of 395 students
* Collected from 2 schools using Questionnaires and the schools grading Administration

```{r}
data_raw <- read.csv("./data/student-mat.csv", 
                 stringsAsFactors = FALSE,
                 check.names=FALSE)
```

```{r}
data <- data_raw %>%
        mutate(Sex = sex,
               StudyTime = studytime-1,
               EducationParents = ifelse(Medu==4 | Fedu==4, 1, 0),
               PaidCourses = ifelse(paid=="yes", 1,0),
               InternetAcces = ifelse(internet=="yes", 1, 0),
               HigherEducationAspiration = ifelse(higher=="yes",1 ,0),
               ExtraEducationalSupport = ifelse(schoolsup=="yes",1,0),
               RegionHouse = address,
               Grade = G3/2) %>%
        select(Sex, StudyTime, EducationParents, PaidCourses, InternetAcces, HigherEducationAspiration, ExtraEducationalSupport, RegionHouse, Grade)
        
```
```{r fig7, out.width="80%", fig.align = "center"}
ggplot(data) + 
  geom_histogram(aes(x=Grade), closed="left", fill="royalblue1", color='white', breaks=seq(0.0,10,0.5), show.legend=TRUE) + 
  labs(title=paste("Distribution of Real Math Grades"), subtitle="By Alex", x="Grade", y="Counts (#)") +
  scale_x_continuous(breaks=seq(0,10,1))
```
* Note he distribution is more spread out than our simulate distribution,
* The cluster at Grade==0 is not proper grading data and should be removed.

```{r}
data <- data %>%
        filter(Grade >= 1.0)
```

#Time for testing the model on the real data
```{r}
fullrun <- 1

priors <- c(set_prior("normal(5.5, 0.5)", class = "b", coef = "Intercept"),
            set_prior("normal(0, 0.5)", class = "b", coef = "PaidCourses"),
            set_prior("normal(0, 0.5)", class = "b", coef = "StudyTime"),
            set_prior("normal(0, 0.5)", class = "b", coef = "ExtraEducationalSupport"),
            set_prior("normal(0, 0.5)", class = "b", coef = "HigherEducationAspiration"),
            set_prior("normal(0, 0.5)", class = "b", coef = "InternetAcces"),
            set_prior("normal(0, 0.5)", class = "b", coef = "EducationParents"),
            set_prior("normal(0, 0.5)", class = "b", coef = "SexM"))

if(fullrun){
  model_math_r <- brm(
              formula = Formula,
              prior = priors,
              data   = data,
              warmup = 500, 
              iter   = 10000, 
              chains = 6, 
              inits  = "random",
              cores  = 6,
              seed = 123,
              backend = "cmdstanr",
              control = list(adapt_delta = 0.99, 
                             max_treedepth = 17),
              silent = TRUE,
              refresh = 0)
  
  saveRDS(model_math_r, "./models/model_demo_r.rds")
} else {
  model_math_r <- readRDS("./models/model_demo_r.rds")
}
```

# Checking our posterior and chains!

```{r fig8, out.width="80%", fig.align = "center"}
model_math_r %>%
  plot(combo = c("hist", "trace"), widths = c(1, 1.5),
       theme = theme_bw(base_size = 10),
       ask   = FALSE
       )
  
```
Again, good mixing, and clean normal estimates for all parameters!
```{r}
summary(model_math_r)
```

```{r}
# now we sample from the model in order to show our predictions
samples_math_r <- tidy_draws(model_math_r) %>%
  dplyr::select(starts_with("b_")) %>%
  pivot_longer(cols = 1:8, names_to = "Model1", values_to = "Value") %>%
  mutate(Feature = as.factor(str_sub(Model1, 3))) %>%
  dplyr::select(Feature, Value)
```

# Bayesian Parameter Estimates on the real data

```{r fig9, out.width="60%", fig.align = "center"}
ggplot(samples_math_r, aes(x=Value, y=Feature, fill=Feature)) +
  geom_density_ridges2(scale = 0.9, 
                       quantile_lines=TRUE,
                       quantile_fun=function(x,...)mean(x)) +
  geom_vline(aes(xintercept = 0), color = "red", linetype = "dashed") +
  scale_fill_viridis_d() +
  theme(legend.position = "none") +
  labs(y = " ", x = "coefficient estimates", title="Bayesian parameter estimates") + 
  scale_x_continuous(breaks=seq(-2,6,0.5))
```
* Most parameters appear to have some effect
* InternetAcces, HigherEducationAspiration and PaidCourses are questionable
* The effect of ExtraEducationalSupport and PaidCourses is negative
Hypothesis: Only 'problematic' students receive this support, Biasing the inference

# How does the model do, predicting on the real data?

```{r}
preds_raw_r <- as_tibble(predict(model_math_r, newdata = data))

preds_R <- cbind(data, preds_raw_r)

plot_data_r = preds_R %>%
            mutate(pred_min=Estimate-Est.Error,
                   pred_max=Estimate+Est.Error)
```

```{r fig11, out.width="60%", fig.align = "center"}
ggplot() +
    geom_errorbar(data = plot_data_r, aes(x = Grade, xend = Grade, ymin=pred_min, ymax = pred_max), size=0.1, width=0.1, color = "red") +
  geom_point(data = plot_data_r, aes(x = Grade, y = Estimate)) +
  geom_abline(intercept = 0, slope = 1, color = "firebrick", linetype = "dashed") +
  labs(y = "Predicted Grade", x = "True Grade", title="Grade vs. Predicted Grade")+
  scale_x_continuous(breaks=seq(1,10,1)) +
  scale_y_continuous(breaks=seq(1,10,1)) +
  coord_fixed()


```

* Hmm, our model has very little predictive power
* As is often the case, real life is more complicated than our toy model

# Appendix

# Updated DAG

```{r fig12, out.width="60%", fig.align = "center"}
dag_r <- dagitty('dag {
bb="-4.361,-3.907,6.053,5.052"
EducationParents [pos="0.165,-2.506"]
ExtraEducationSupport [pos="-2.624,0.509"]
FirstTrimesterGrade [pos="-2.368,3.880"]
Grade [outcome,pos="0.128,0.557"]
HigherEducationAspiration [pos="-1.235,-0.496"]
InternetAcces [pos="-0.979,-1.746"]
PaidCourses [pos="-0.488,2.606"]
RegionHouse [pos="-2.460,-2.562"]
Sex [pos="2.442,-1.390"]
StudyTime [pos="-2.019,2.583"]
EducationParents -> Grade
EducationParents -> HigherEducationAspiration
ExtraEducationSupport -> Grade
ExtraEducationSupport -> PaidCourses
FirstTrimesterGrade -> ExtraEducationSupport
FirstTrimesterGrade -> Grade
FirstTrimesterGrade -> PaidCourses
HigherEducationAspiration -> ExtraEducationSupport
HigherEducationAspiration -> Grade
HigherEducationAspiration -> PaidCourses
HigherEducationAspiration -> StudyTime
InternetAcces -> Grade
PaidCourses -> Grade
RegionHouse -> EducationParents
RegionHouse -> HigherEducationAspiration
RegionHouse -> InternetAcces
Sex -> Grade
Sex -> HigherEducationAspiration
StudyTime -> ExtraEducationSupport
StudyTime -> Grade
StudyTime -> PaidCourses
}
')
plot(dag_r)
```

* What happens when we include the start of the year grade? Will it fix our problems?
* When we don't include FirstTrimesterGrade, it's influence flows via ExtraEducationSupport and PaidCourses.

```{r, results='hide'}
data <- data_raw %>%
        mutate(Sex = sex,
               StudyTime = studytime-1,
               EducationParents = ifelse(Medu==4 | Fedu==4, 1, 0),
               PaidCourses = ifelse(paid=="yes", 1,0),
               InternetAcces = ifelse(internet=="yes", 1, 0),
               HigherEducationAspiration = ifelse(higher=="yes",1 ,0),
               ExtraEducationalSupport = ifelse(schoolsup=="yes",1,0),
               RegionHouse = address,
               Grade = G3/2,
               GradeFirstTrimester = G1/2) %>%
        select(Sex, StudyTime, EducationParents, PaidCourses, InternetAcces, HigherEducationAspiration, ExtraEducationalSupport, RegionHouse, Grade, GradeFirstTrimester) %>%
        filter(Grade>=1.0)

Formula_r <- Grade ~ 0  + Intercept + 
                        PaidCourses +
                        StudyTime +
                        ExtraEducationalSupport +
                        HigherEducationAspiration +
                        InternetAcces + 
                        EducationParents +
                        Sex +
                        GradeFirstTrimester

priors_r <- c(set_prior("normal(5.5, 0.5)", class = "b", coef = "Intercept"),
            set_prior("normal(0, 0.5)", class = "b", coef = "PaidCourses"),
            set_prior("normal(0, 0.5)", class = "b", coef = "StudyTime"),
            set_prior("normal(0, 0.5)", class = "b", coef = "ExtraEducationalSupport"),
            set_prior("normal(0, 0.5)", class = "b", coef = "HigherEducationAspiration"),
            set_prior("normal(0, 0.5)", class = "b", coef = "InternetAcces"),
            set_prior("normal(0, 0.5)", class = "b", coef = "EducationParents"),
            set_prior("normal(0, 0.5)", class = "b", coef = "SexM"),
            set_prior("normal(0, 0.5)", class = "b", coef = "GradeFirstTrimester"))


model_math_r2 <- brm(
            formula = Formula_r,
            prior = priors_r,
            data   = data,
            warmup = 500, 
            iter   = 10000, 
            chains = 6, 
            inits  = "random",
            cores  = 6,
            seed = 123,
            backend = "cmdstanr",
            control = list(adapt_delta = 0.99, 
                           max_treedepth = 17),
            silent = TRUE,
            refresh = 0)

saveRDS(model_math_r2, "./models/model_demo_r.rds")


preds_raw_r2 <- as_tibble(predict(model_math_r2, newdata = data))

preds_r2 <- cbind(data, preds_raw_r2)

plot_data_r2 = preds_r2 %>%
            mutate(pred_min=Estimate-Est.Error,
                   pred_max=Estimate+Est.Error)
```

# Better, but not good enough

```{r fig13, out.width="70%", fig.align = "center"}
ggplot() +
    geom_errorbar(data = plot_data_r2, aes(x = Grade, xend = Grade, ymin=pred_min, ymax = pred_max), size=0.1, width=0.1, color = "red") +
  geom_point(data = plot_data_r2, aes(x = Grade, y = Estimate)) +
  geom_abline(intercept = 0, slope = 1, color = "firebrick", linetype = "dashed") +
  labs(y = "Predicted Grade", x = "True Grade", title="Grade vs. Predicted Grade")+
  scale_x_continuous(breaks=seq(1,10,1)) +
  scale_y_continuous(breaks=seq(1,10,1)) +
  coord_fixed()
```

* As expected, previous grades are a good indication of the final grade.
* Other than that, the model is clearly broken. More work is needed!
